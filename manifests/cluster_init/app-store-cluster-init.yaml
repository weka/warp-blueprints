apiVersion: warp.io/v1alpha1
kind: WekaAppStore
metadata:
  name: app-store-cluster-init
  namespace: default
spec:
  appStack:
    components:
      - name: kube-prom-stack
        description: "Prometheus and Grafana monitoring stack"
        enabled: true
        dependsOn:
          - prometheus-adapter
        helmChart:
          name: "https://prometheus-community.github.io/helm-charts"
          releaseName: "kube-prometheus-stack"
        valuesFiles:
          - kind: ConfigMap
            name: prom-stack-config
            key: prom-stack_values.yaml
        waitForReady: true
        readinessCheck:
          type: pod
          selector: "app.kubernetes.io/instance=kube-prometheus-stack,app.kubernetes.io/name=grafana"
          timeout: 300
        targetNamespace: monitoring
        #
      - name: prometheus-adapter
        description: "Prometheus Adapter for Kubernetes metrics APIs"
        enabled: true
        helmChart:
          name: "https://prometheus-community.github.io/helm-charts"
          releaseName: "prometheus-adapter"
        valuesFiles:
          - kind: ConfigMap
            name: prometheus-adapter-config
            key: prometheus-adapter_values.yaml
        waitForReady: true
        readinessCheck:
          type: pod
          selector: "app.kubernetes.io/instance=prometheus-adapter,app.kubernetes.io/name=prometheus-adapter"
          timeout: 300
        targetNamespace: monitoring

      #NVIDIA GPU Operator
      - name: nvidia-operator
        description: "Certificate management for Kubernetes"
        enabled: true
        dependsOn:
          - prometheus-adapter
        helmChart:
          name: "https://nvidia.github.io/gpu-operator"
          version: "v25.3.0"
          releaseName: "gpu-operator"
        waitForReady: true
        readinessCheck:
          type: pod
          selector: "app=gpu-operator"
          timeout: 300
        targetNamespace: gpu-operator
      
      # Install NVIDIA DCGM ServiceMonitor via operator manifests
      - name: nvidia-dcgm-servicemonitor
        description: "ServiceMonitor for NVIDIA DCGM exporter"
        enabled: true
        dependsOn:
          - kube-prom-stack        # ensure Prometheus Operator (CRDs) is ready
          - nvidia-operator        # ensure DCGM exporter Service exists
        kubernetesManifest: |
          apiVersion: monitoring.coreos.com/v1
          kind: ServiceMonitor
          metadata:
            name: dcgm-exporter
            namespace: monitoring
            labels:
              release: kube-prom-stack
          spec:
            namespaceSelector:
              matchNames:
                - gpu-operator
            selector:
              matchLabels:
                app: nvidia-dcgm-exporter
            endpoints:
              - port: gpu-metrics
                path: /metrics
                interval: 30s
                honorLabels: true
---
# ConfigMap for Prom-stack values
apiVersion: v1
kind: ConfigMap
metadata:
  name: prom-stack-config
  namespace: monitoring
data:
  prom-stack_values.yaml: |
    ## Create default rules for monitoring the cluster
    #
    # Disable `etcd` and `kubeScheduler` rules (managed by DOKS, so metrics are not accessible)
    defaultRules:
      create: true
      rules:
        etcd: false
        kubeScheduler: false

    ## Component scraping kube scheduler
    ##
    # Disabled because it's being managed by DOKS, so it's not accessible
    kubeScheduler:
      enabled: false

    ## Component scraping etcd
    ##
    # Disabled because it's being managed by DOKS, so it's not accessible
    kubeEtcd:
      enabled: false

    alertmanager:
      ## Deploy alertmanager
      ##
      enabled: true
      # config:
      #   global:
      #     resolve_timeout: 5m
      #     slack_api_url: "<YOUR_SLACK_APP_INCOMING_WEBHOOK_URL_HERE>"
      #   route:
      #     receiver: "slack-notifications"
      #     repeat_interval: 12h
      #     routes:
      #       - receiver: "slack-notifications"
      #   receivers:
      #     - name: "slack-notifications"
      #       slack_configs:
      #         - channel: "#<YOUR_SLACK_CHANNEL_NAME_HERE>"
      #           send_resolved: true
      #           title: "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}"
      #           text: "{{ range .Alerts }}{{ .Annotations.description }}\n{{ end }}"

    ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
    ##
    grafana:
      enabled: true
      adminPassword: prom-operator # Please change the default password in production !!!
      # persistence / affinity omitted for brevity

    ## Manages Prometheus and Alertmanager components
    ##
    prometheusOperator:
      enabled: true

    ## Cluster-wide node & kubelet metrics
    ##
    nodeExporter:
      enabled: true

    kubelet:
      enabled: true
      serviceMonitor:
        enabled: true
        # Enable cAdvisor metrics if your chart supports this flag
        # cAdvisor: true

    ## Deploy a Prometheus instance
    ##
    prometheus:
      enabled: true

      ## IMPORTANT: enable cluster-wide monitoring
      prometheusSpec:
        # Allow Prometheus to discover ServiceMonitors/PodMonitors in ALL namespaces
        serviceMonitorNamespaceSelector: {}
        podMonitorNamespaceSelector: {}

        # Do not restrict by labels â€“ pick up all ServiceMonitors/PodMonitors
        serviceMonitorSelector: {}
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelector: {}
        podMonitorSelectorNilUsesHelmValues: false
---
# ConfigMap for Prometheus Adaptor values
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-adapter-config
  namespace: monitoring
data:
  prometheus-adapter_values.yaml: |
    loglevel: 1

    prometheus:
      url: http://kube-prom-stack-kube-prome-prometheus.monitoring.svc
      port: 9090

    rules:
      default: true
      custom:

      # Example metric to export for HPA
      - seriesQuery: '{__name__=~"^vllm:num_requests_waiting$"}'
        resources:
          overrides:
            namespace:
              resource: "namespace"
        name:
          matches: ""
          as: "vllm_num_requests_waiting"
        metricsQuery: sum by(namespace) (vllm:num_requests_waiting)

      # Export num_incoming_requests_total by model name
      - seriesQuery: '{__name__=~"^vllm:num_incoming_requests_total$"}'
        resources:
          overrides:
            namespace:
              resource: "namespace"
        name:
          matches: ""
          as: "vllm_num_incoming_requests_total"
        metricsQuery: sum by(namespace, model) (vllm:num_incoming_requests_total)
---
#apiVersion: storage.k8s.io/v1
#kind: StorageClass
#metadata:
#  name: default
#  annotations:
#    storageclass.kubernetes.io/is-default-class: "true"
#provisioner: kubernetes.io/aws-ebs
#reclaimPolicy: Delete
#volumeBindingMode: WaitForFirstConsumer #This was changed to address the multi AZ nature of EKS
#parameters:
#  type: gp3
