apiVersion: warp.io/v1alpha1
kind: WekaAppStore
metadata:
  name: [[ deployment_name | default('openfold-stack') ]]
  namespace: [[ namespace | default('openfold') ]]
spec:
  appStack:
    components:
        # 1. argo - workflow engine for managing Openfold jobs
      - name: argo-scheduler
        description: "Job controller for Openfold CPU and GPU workloads"
        enabled: true
        helmChart:
          name: "https://argoproj.github.io/argo-helm"
          version: "0.41.8"
          releaseName: "argo-workflows"
        valuesFiles:
          - kind: ConfigMap
            name: argo-config
            key: argo_values.yaml
        waitForReady: true
        readinessCheck:
          type: pod
          selector: "app.kubernetes.io/instance=argo-workflows"
          timeout: 300
        targetnamespace: [[ namespace | default('openfold') ]]

      #2. Storage Class for Openfold data
      - name: openfold-storage-class
        description: "Storage Class for Openfold data"
        enabled: true
        dependsOn:
          - argo-scheduler
        kubernetesManifest: |
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: weka-openfold-dir-api
          provisioner: csi.weka.io
          
          # For production I’d usually keep data by default
          reclaimPolicy: Retain
          volumeBindingMode: Immediate
          allowVolumeExpansion: true
          
          parameters:
            # Directory-backed volumes on an existing WEKA filesystem
            volumeType: dir/v1
            filesystemName: [[ weka_cluster_filesystem | default('default') ]]
          
            # Enforce WEKA directory quota strictly (HARD or SOFT)
            capacityEnforcement: HARD
          
            # Optional: set ownership / permissions on the root of each volume
            ownerUid: "1000"
            ownerGid: "1000"
            permissions: "0775"
          
            # CSI API secret wiring – names/namespaces MUST exist in your cluster
            csi.storage.k8s.io/provisioner-secret-name: &secretName csi-wekafs-api-secret
            csi.storage.k8s.io/provisioner-secret-namespace: &secretNamespace csi-wekafs
          
            csi.storage.k8s.io/controller-publish-secret-name: *secretName
            csi.storage.k8s.io/controller-publish-secret-namespace: *secretNamespace
            csi.storage.k8s.io/controller-expand-secret-name: *secretName
            csi.storage.k8s.io/controller-expand-secret-namespace: *secretNamespace
            csi.storage.k8s.io/node-stage-secret-name: *secretName
            csi.storage.k8s.io/node-stage-secret-namespace: *secretNamespace
            csi.storage.k8s.io/node-publish-secret-name: *secretName
            csi.storage.k8s.io/node-publish-secret-namespace: *secretNamespace

        waitForReady: false
        readinessCheck:
          type: pod
          selector: ""
          timeout: 300
        targetnamespace: [[ namespace | default('openfold') ]]

      #3. Create the PVC for Openfold data
      - name: openfold-data-pvc
        description: "PersistentVolumeClaim for Openfold data storage"
        enabled: true
        dependsOn:
          - openfold-storage-class
        kubernetesManifest: |
          apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            name: openfold-data-pvc
            namespace: [[ namespace | default('openfold') ]]
          spec:
            accessModes:
              - ReadWriteMany
            resources:
              requests:
                storage: [[ openfold_storage_capacity | default('1000Gi') ]]
            storageClassName: weka-openfold-dir-api

        waitForReady: false
        readinessCheck:
          type: pod
          selector: ""
          timeout: 300
        targetNamespace: [[ namespace | default('openfold') ]]

      # 6. Argo workflow template for Openfold pipeline
      - name: openfold-workflow-template
        description: "Argo Workflow Template for Openfold Protein Structure Prediction"
        enabled: true
        dependsOn:
          - argo-scheduler
          - openfold-data-pvc
          - openfold-storage-class
        kubernetesManifest: |
          apiVersion: argoproj.io/v1alpha1
          kind: WorkflowTemplate
          metadata:
            name: openfold-pipeline
            namespace: [[ namespace | default('openfold') ]]
          spec:
            entrypoint: openfold-pipeline
            serviceAccountName: argo-workflow-sa
          
            # ---------------------------------------------------------------------------
            # Top-level parameters (editable in Argo UI) with defaults
            # ---------------------------------------------------------------------------
            arguments:
              parameters:
                # Legacy single-protein parameter (still here if you want to use it directly)
                - name: protein-id
                  value: "protein1.fasta"
                  description: "FASTA filename or protein ID under FASTA root (e.g. protein1.fasta)."
          
                # NEW: list of protein IDs to process in parallel
                - name: protein-ids
                  value: '["protein1.fasta"]'
                  description: "JSON array of FASTA filenames or protein IDs to process in parallel."
          
                # CPU / MMseqs2-related
                - name: uniref50-db
                  value: "/data/databases/uniref50/uniref50_mmseqs"
                  description: "MMseqs2 UniRef50 DB prefix (no extension)."
          
                - name: pdb70-db
                  value: "/data/databases/pdb70/pdb70"
                  description: "pdb70 HHM database prefix (no extension)."
          
                - name: mmseqs-bin
                  value: "/usr/local/bin/mmseqs-latest"
                  description: "Path to the mmseqs2 binary inside the CPU container."
          
                - name: mmseqs-threads
                  value: "32"
                  description: "Number of CPU threads for MMseqs2 (--mmseqs-threads)."
          
                - name: mmseqs-db-load-mode
                  value: "2"
                  description: "MMseqs2 --db-load-mode (2 = read directly from storage)."
          
                - name: search-evalue
                  value: "1.0"
                  description: "MMseqs2 search E-value cutoff (-e). Higher = more hits."
          
                - name: min-hit-cov
                  value: "0.3"
                  description: "Minimum query coverage (-c) for hits (0–1)."
          
                - name: result2msa-max-seq-id
                  value: "0.95"
                  description: "result2msa --max-seq-id (0–1). Higher = more similar sequences kept."
          
                # GPU / OpenFold-related – defaults mirror your GPU script
                - name: fasta-root
                  value: "/data/input_fasta_single"
                  description: "Root directory containing protein FASTA files (--fasta-root)."
          
                - name: output-root
                  value: "/data/predictions"
                  description: "Root directory for model prediction outputs (--output-root)."
          
                - name: precomputed-alignments-dir
                  value: "/data/databases/embeddings_output_dir"
                  description: "Where CPU step wrote precomputed A3M/HHR files (--precomputed-alignments-dir)."
          
                - name: mmcif-dir
                  value: "/data/databases/pdb_mmcif/mmcif_files"
                  description: "Directory containing PDB mmCIF files (--mmcif-dir)."
          
                - name: openfold-checkpoint-path
                  value: "/data/openfold_weights/finetuning_ptm_1.pt"
                  description: "OpenFold checkpoint path (--openfold-checkpoint-path)."
          
                - name: config-preset
                  value: "model_1_ptm"
                  description: "OpenFold config preset (--config-preset)."
          
                - name: model-device
                  value: "cuda:0"
                  description: "Model device, e.g. cuda:0 (--model-device)."
          
                - name: gpu-id
                  value: ""
                  description: "Optional GPU ID for CUDA_VISIBLE_DEVICES (--gpu-id). Leave empty to use default visibility."
          
                - name: run-pretrained-script
                  value: "/app/run_pretrained_openfold.py"
                  description: "Path to run_pretrained_openfold.py inside the container (--run-pretrained-script)."
          
            # ---------------------------------------------------------------------------
            # DAG: CPU (MMseqs2+HHsearch) → GPU (OpenFold), per-protein in parallel
            # ---------------------------------------------------------------------------
            templates:
              - name: openfold-pipeline
                dag:
                  tasks:
                    # Fan-out CPU step: one mmseq2-cpu per protein-id in protein-ids
                    - name: mmseq2-cpu
                      template: mmseq2-cpu
                      arguments:
                        parameters:
                          - name: protein-id
                            value: "{{item}}"
                          - name: uniref50-db
                            value: "{{workflow.parameters.uniref50-db}}"
                          - name: pdb70-db
                            value: "{{workflow.parameters.pdb70-db}}"
                          - name: mmseqs-bin
                            value: "{{workflow.parameters.mmseqs-bin}}"
                          - name: mmseqs-threads
                            value: "{{workflow.parameters.mmseqs-threads}}"
                          - name: mmseqs-db-load-mode
                            value: "{{workflow.parameters.mmseqs-db-load-mode}}"
                          - name: search-evalue
                            value: "{{workflow.parameters.search-evalue}}"
                          - name: min-hit-cov
                            value: "{{workflow.parameters.min-hit-cov}}"
                          - name: result2msa-max-seq-id
                            value: "{{workflow.parameters.result2msa-max-seq-id}}"
                      withParam: "{{workflow.parameters.protein-ids}}"
          
                    # Fan-out GPU step: one gpu-openfold per protein-id, each depending on its mmseq2-cpu
                    - name: gpu-openfold
                      template: gpu-openfold
                      dependencies: [mmseq2-cpu]
                      arguments:
                        parameters:
                          - name: protein-id
                            value: "{{item}}"
                          - name: fasta-root
                            value: "{{workflow.parameters.fasta-root}}"
                          - name: output-root
                            value: "{{workflow.parameters.output-root}}"
                          - name: precomputed-alignments-dir
                            value: "{{workflow.parameters.precomputed-alignments-dir}}"
                          - name: mmcif-dir
                            value: "{{workflow.parameters.mmcif-dir}}"
                          - name: openfold-checkpoint-path
                            value: "{{workflow.parameters.openfold-checkpoint-path}}"
                          - name: config-preset
                            value: "{{workflow.parameters.config-preset}}"
                          - name: model-device
                            value: "{{workflow.parameters.model-device}}"
                          - name: gpu-id
                            value: "{{workflow.parameters.gpu-id}}"
                          - name: run-pretrained-script
                            value: "{{workflow.parameters.run-pretrained-script}}"
                      withParam: "{{workflow.parameters.protein-ids}}"
          
              # -----------------------------------------------------------------------
              # CPU template – aligned to run_mmseq_pipeline.py
              # -----------------------------------------------------------------------
              - name: mmseq2-cpu
                inputs:
                  parameters:
                    - name: protein-id
                    - name: uniref50-db
                    - name: pdb70-db
                    - name: mmseqs-bin
                    - name: mmseqs-threads
                    - name: mmseqs-db-load-mode
                    - name: search-evalue
                    - name: min-hit-cov
                    - name: result2msa-max-seq-id
                container:
                  image: wekachrisjen/warp-openfold-cpu:v0.18
                  imagePullPolicy: IfNotPresent
                  command: ["bash", "-c"]
                  args:
                    - |
                      set -euo pipefail
          
                      echo "[CPU] Running run_mmseq_pipeline.py for {{inputs.parameters.protein-id}}"
          
                      python3 /app/run_mmseq_pipeline.py \
                        --protein-id "{{inputs.parameters.protein-id}}" \
                        --fasta-root /data/input_fasta_single \
                        --precomputed-alignments-dir /data/databases/embeddings_output_dir \
                        --uniref50-db "{{inputs.parameters.uniref50-db}}" \
                        --tmp-dir /data/tmp \
                        --mmseqs-bin "{{inputs.parameters.mmseqs-bin}}" \
                        --mmseqs-threads {{inputs.parameters.mmseqs-threads}} \
                        --mmseqs-db-load-mode {{inputs.parameters.mmseqs-db-load-mode}} \
                        --search-evalue {{inputs.parameters.search-evalue}} \
                        --min-hit-cov {{inputs.parameters.min-hit-cov}} \
                        --result2msa-max-seq-id {{inputs.parameters.result2msa-max-seq-id}} \
                        --hhsearch-bin hhsearch \
                        --pdb70-db "{{inputs.parameters.pdb70-db}}"
          
                  resources:
                    requests:
                      cpu: "8"
                      memory: "32Gi"
                    limits:
                      cpu: "32"
                      memory: "64Gi"
          
                  volumeMounts:
                    - name: openfold-data
                      mountPath: /data
          
                volumes:
                  - name: openfold-data
                    persistentVolumeClaim:
                      claimName: openfold-data-pvc
          
              # -----------------------------------------------------------------------
              # GPU template – aligned to run_gpu_inference.py
              # -----------------------------------------------------------------------
              - name: gpu-openfold
                inputs:
                  parameters:
                    - name: protein-id
                    - name: fasta-root
                    - name: output-root
                    - name: precomputed-alignments-dir
                    - name: mmcif-dir
                    - name: openfold-checkpoint-path
                    - name: config-preset
                    - name: model-device
                    - name: gpu-id
                    - name: run-pretrained-script
                container:
                  image: wekachrisjen/warp-openfold-gpu:v0.22
                  imagePullPolicy: IfNotPresent
                  command: ["/opt/conda/envs/gpu-env/bin/python"]
                  args:
                    - /app/run_gpu_inference.py
                    - --protein-id
                    - "{{inputs.parameters.protein-id}}"
                    - --fasta-root
                    - "{{inputs.parameters.fasta-root}}"
                    - --output-root
                    - "{{inputs.parameters.output-root}}"
                    - --precomputed-alignments-dir
                    - "{{inputs.parameters.precomputed-alignments-dir}}"
                    - --mmcif-dir
                    - "{{inputs.parameters.mmcif-dir}}"
                    - --openfold-checkpoint-path
                    - "{{inputs.parameters.openfold-checkpoint-path}}"
                    - --config-preset
                    - "{{inputs.parameters.config-preset}}"
                    - --model-device
                    - "{{inputs.parameters.model-device}}"
                    - --run-pretrained-script
                    - "{{inputs.parameters.run-pretrained-script}}"
                    # gpu-id is optional, you can let run_gpu_inference.py handle it
                    # via an optional arg if you want.
          
                  resources:
                    limits:
                      nvidia.com/gpu: 1
                      cpu: "8"
                      memory: "32Gi"
                    requests:
                      nvidia.com/gpu: 1
                      cpu: "4"
                      memory: "16Gi"
          
                  volumeMounts:
                    - name: openfold-data
                      mountPath: /data
          
                volumes:
                  - name: openfold-data
                    persistentVolumeClaim:
                      claimName: openfold-data-pvc

        waitForReady: false
        readinessCheck:
          type: pod
          selector: ""
          timeout: 300
        targetNamespace: [[ namespace | default('openfold') ]]

      # 7. Argo workflow template for bootstraping databases
      - name: openfold-workflow-bootstrap-template
        description: "Argo Workflow Template for Openfold Database Bootstraping"
        enabled: true
        dependsOn:
          - argo-scheduler
          - openfold-data-pvc
          - openfold-storage-class
        kubernetesManifest: |
          apiVersion: argoproj.io/v1alpha1
          kind: WorkflowTemplate
          metadata:
            name: openfold-db-bootstrap
            namespace: [[ namespace | default('openfold') ]]
          spec:
            entrypoint: bootstrap-databases
          
            # User-editable parameters (appear in Argo UI)
            arguments:
              parameters:
                - name: pdb70-root
                  value: "/data/databases/pdb70"
                - name: pdb70-url
                  value: "https://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/pdb70_from_mmcif_latest.tar.gz"
                # Which UniRef preset to use with `mmseqs databases`
                - name: uniref-source
                  value: "UniRef50"
                # Local directory name under /data/databases
                - name: uniref-name
                  value: "uniref50"
          
            # Optional: cap overall concurrent pods for this workflow
            parallelism: 4
          
            volumes:
              - name: openfold-data
                persistentVolumeClaim:
                  claimName: openfold-data-pvc
          
            templates:
              # ============================
              # Top-level: run PDB70 + UniRef in parallel
              # ============================
              - name: bootstrap-databases
                steps:
                  - - name: download-pdb70
                      template: download-pdb70
                    - name: build-uniref
                      template: build-uniref
          
              # ============================
              # PDB70 downloader (1 pod)
              # ============================
              - name: download-pdb70
                container:
                  image: wekachrisjen/warp-openfold-cpu:v0.5
                  imagePullPolicy: IfNotPresent
                  command: ["bash", "-c"]
                  args:
                    - |
                      set -euo pipefail
          
                      log() {
                        echo "[$(date '+%Y-%m-%d %H:%M:%S')] [INIT-PDB70] $*" >&2
                      }
          
                      PDB70_ROOT="{{workflow.parameters.pdb70-root}}"
                      PDB70_URL="{{workflow.parameters.pdb70-url}}"
                      PDB70_TARBALL="pdb70_from_mmcif_latest.tar.gz"
          
                      log "Ensuring PDB70 root directory exists at ${PDB70_ROOT}"
                      mkdir -p "${PDB70_ROOT}"
                      cd "${PDB70_ROOT}"
          
                      # Already present?
                      if ls pdb70_cs219.ffdata >/dev/null 2>&1; then
                        log "PDB70 already installed. Nothing to do."
                        exit 0
                      fi
          
                      log "PDB70 not found. Downloading from ${PDB70_URL}"
          
                      if command -v curl >/dev/null 2>&1; then
                        curl -L -o "${PDB70_TARBALL}" "${PDB70_URL}"
                      else
                        wget -O "${PDB70_TARBALL}" "${PDB70_URL}"
                      fi
          
                      log "Extracting PDB70 tarball..."
                      tar -xzf "${PDB70_TARBALL}"
          
                      log "Locating pdb70_cs219.ffdata..."
                      inner_file="$(find "${PDB70_ROOT}" -maxdepth 3 -type f -name 'pdb70_cs219.ffdata' | head -n 1 || true)"
          
                      if [ -z "${inner_file}" ]; then
                        log "ERROR: Could not locate pdb70_cs219.ffdata after extracting tarball."
                        exit 1
                      fi
          
                      inner_dir="$(dirname "${inner_file}")"
                      log "Found pdb70 files inside ${inner_dir}"
          
                      # Symlink all pdb70_* files into the root directory
                      for f in "${inner_dir}"/pdb70_*; do
                        base="$(basename "${f}")"
                        if [ ! -e "${PDB70_ROOT}/${base}" ]; then
                          ln -s "${f}" "${PDB70_ROOT}/${base}"
                          log "Linked ${base}"
                        fi
                      done
          
                      if [ ! -f "${PDB70_ROOT}/pdb70_cs219.ffdata" ]; then
                        log "ERROR: PDB70 installation incomplete."
                        exit 1
                      fi
          
                      log "PDB70 installed successfully at ${PDB70_ROOT}"
                  volumeMounts:
                    - name: openfold-data
                      mountPath: /data
          
              # ============================
              # UniRef MMseqs DB builder (1 pod)
              # Uses user-selected UniRef DB and local name
              # ============================
              - name: build-uniref
                container:
                  image: wekachrisjen/warp-openfold-cpu:v0.5
                  imagePullPolicy: IfNotPresent
                  env:
                    - name: MMSEQS_THREADS
                      value: "32"
                  command: ["bash", "-c"]
                  args:
                    - |
                      set -euo pipefail
          
                      log() {
                        echo "[$(date '+%Y-%m-%d %H:%M:%S')] [INIT-UNIREF] $*" >&2
                      }
          
                      UNIREFF_SOURCE="{{workflow.parameters.uniref-source}}"
                      UNIREFF_NAME="{{workflow.parameters.uniref-name}}"
          
                      DB_DIR="/data/databases/${UNIREFF_NAME}"
                      DB_PREFIX="${DB_DIR}/${UNIREFF_NAME}_mmseqs"
          
                      log "Using UniRef source preset: ${UNIREFF_SOURCE}"
                      log "Target directory: ${DB_DIR}"
                      log "Target MMseqs DB prefix: ${DB_PREFIX}"
          
                      mkdir -p "${DB_DIR}" /data/tmp
          
                      # If DB already exists, skip rebuild
                      if ls "${DB_PREFIX}".dbtype >/dev/null 2>&1; then
                        log "MMseqs DB '${DB_PREFIX}' already exists. Skipping build."
                        exit 0
                      fi
          
                      log "Building ${UNIREFF_SOURCE} MMseqs database..."
                      mmseqs databases "${UNIREFF_SOURCE}" \
                        "${DB_PREFIX}" \
                        /data/tmp \
                        --threads "${MMSEQS_THREADS:-32}"
          
                      log "${UNIREFF_SOURCE} MMseqs database build complete at ${DB_PREFIX}"
                  volumeMounts:
                    - name: openfold-data
                      mountPath: /data

        waitForReady: false
        readinessCheck:
          type: pod
          selector: ""
          timeout: 300
        targetNamespace: [[ namespace | default('openfold') ]]

      # 8. Kubernetes Role for Argo workflows
      - name: kubernetes-role
        description: "Kubernetes Role for Argo Workflows"
        enabled: true
        dependsOn:
        kubernetesManifest: |
          apiVersion: rbac.authorization.k8s.io/v1
          kind: Role
          metadata:
            name: argo-workflow-role
            namespace: [[ namespace | default('openfold') ]]
          rules:
            # Argo needs to create & manage pods that run each step
            - apiGroups: [""]
              resources: ["pods", "pods/log"]
              verbs: ["get", "watch", "list", "create", "update", "patch", "delete"]
          
            # Usually helpful so workflows can manage their own PVCs & ConfigMaps
            - apiGroups: [""]
              resources: ["persistentvolumeclaims", "configmaps"]
              verbs: ["get", "watch", "list", "create", "update", "patch", "delete"]
          
            # Optional but common: manage workflow CRDs in this namespace
            - apiGroups: ["argoproj.io"]
              resources: ["workflows", "workflowtemplates", "cronworkflows"]
              verbs: ["get", "watch", "list", "create", "update", "patch", "delete"]

        waitForReady: false
        readinessCheck:
          type: pod
          selector: ""
          timeout: 300
        targetNamespace: [[ namespace | default('openfold') ]]

      # 9. Kubernetes RoleBinding for Argo workflows
      - name: kubernetes-role-binding
        description: "Kubernetes RoleBinding for Argo Workflows"
        enabled: true
        dependsOn:
            - kubernetes-role
        kubernetesManifest: |
          apiVersion: rbac.authorization.k8s.io/v1
          kind: RoleBinding
          metadata:
            name: argo-workflow-rolebinding
            namespace: [[ namespace | default('openfold') ]]
          subjects:
            - kind: ServiceAccount
              name: argo-workflow-sa          #matches your Helm values
              Namespace: [[ namespace | default('openfold') ]]
          roleRef:
            kind: Role
            name: argo-workflow-role
            apiGroup: rbac.authorization.k8s.io

        waitForReady: false
        readinessCheck:
          type: pod
          selector: ""
          timeout: 300
        targetNamespace: [[ namespace | default('openfold') ]]
---
# ConfigMap for Argo values
apiVersion: v1
kind: ConfigMap
metadata:
  name: argo-config
  namespace: [[ namespace | default('openfold') ]]
data:
  argo_values.yaml: |
    ## Setup Argo Server, can change to sso or oidc if needed
    server:
      enabled: true
      authMode: "server"
      
    # This SA only works when the workflow is deployed in same k8s namespace as Argo Server
    workflow:
      serviceAccount:
        create: true
        name: argo-workflow-sa
    
    artifactRepository:
      pvc:
        create: true
        size: 100Gi
        storageClassName: "weka-openfold-dir-api"
---
